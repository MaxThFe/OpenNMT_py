

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="EN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="EN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Supervised Finetuning of llama 7B to replicate Vicuna &mdash; OpenNMT-py  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Build Vocab" href="../../options/build_vocab.html" />
    <link rel="prev" title="Gated Graph Neural Networks" href="../ggnn/GGNN.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> OpenNMT-py
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changes.html">Versions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../CONTRIBUTING.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../ref.html">References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Frequently Asked Questions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html">How do I use my v2 models in v3 ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-do-i-train-the-transformer-model">How do I train the Transformer model?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#performance-tips">Performance tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#position-encoding-absolute-vs-relative-vs-rotary-embeddings-vs-alibi">Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#do-you-support-multi-gpu">Do you support multi-gpu?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-do-i-use-pretrained-embeddings-e-g-glove">How do I use Pretrained embeddings (e.g. GloVe)?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-can-i-ensemble-models-at-inference">How can I ensemble Models at inference?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-can-i-weight-different-corpora-at-training">How can I weight different corpora at training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#what-special-tokens-does-opennmt-py-use">What special tokens does OpenNMT-py use?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-can-i-apply-on-the-fly-tokenization-and-subword-regularization-when-training">How can I apply on-the-fly tokenization and subword regularization when training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms">What are the readily available on-the-fly data transforms?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-can-i-create-custom-on-the-fly-data-transforms">How can I create custom on-the-fly data transforms?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-to-use-lora-and-8bit-loading-to-finetune-a-big-model">How to use LoRa and 8bit loading to finetune a big model ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-to-use-gradient-checkpointing-when-dealing-with-a-big-model">How to use gradient checkpointing when dealing with a big model ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#can-i-get-word-alignments-while-translating">Can I get word alignments while translating?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-can-i-update-a-checkpoint-s-vocabulary">How can I update a checkpoint’s vocabulary?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-can-i-use-source-word-features">How can I use source word features?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../FAQ.html#how-can-i-set-up-a-translation-server">How can I set up a translation server ?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../wmt17/Translation.html">Translation WMT17 en-de</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wiki_103/LanguageModelGeneration.html">Language Model Wiki-103</a></li>
<li class="toctree-l1"><a class="reference internal" href="../summary/Summarization.html">Summarization CNN/DM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ggnn/GGNN.html">Gated Graph Neural Networks</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Supervised Finetuning of llama 7B to replicate Vicuna</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#dependencies">Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#data">Data</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#checkpoints">Checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vocabulary">Vocabulary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#datasets">Datasets</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#finetuning">Finetuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inference">Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#concatenation-of-the-checkpoints">Concatenation of the checkpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="#input-examples">Input examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference-with-translate-py">Inference with <code class="docutils literal notranslate"><span class="pre">translate.py</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#inference-with-ctranslate">Inference with <code class="docutils literal notranslate"><span class="pre">CTranslate</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Scripts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../options/build_vocab.html">Build Vocab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../options/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../options/translate.html">Translate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../options/server.html">Server</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../onmt.html">Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onmt.modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onmt.translation.html">Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onmt.translate.translation_server.html">Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onmt.inputters.html">Data Loaders</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Legacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../legacy/FAQ.html">FAQ (Legacy version)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../legacy/im2text.html">Image to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../legacy/speech2text.html">Speech to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../legacy/vid2text.html">Video to Text</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">OpenNMT-py</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>Supervised Finetuning of llama 7B to replicate Vicuna</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/examples/replicate_vicuna/ReplicateVicuna.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="supervised-finetuning-of-llama-7b-to-replicate-vicuna">
<h1>Supervised Finetuning of llama 7B to replicate Vicuna<a class="headerlink" href="#supervised-finetuning-of-llama-7b-to-replicate-vicuna" title="Permalink to this heading">¶</a></h1>
<p>This tutorial shows how to finetune a LLaMA 7B foundation model on instruction data including multi-round conversations.</p>
<p>Different features will be enabled:</p>
<ul class="simple">
<li><p>Application of the LoRa method to the attention layers.</p></li>
<li><p>8bit compression of the position-wise feed-forward layers.</p></li>
<li><p>Architectural improvements used during the training of the llama models (RMS normalisation, Rotary Embeddings, SwiGLU activation).</p></li>
</ul>
<p>The maximal context length will be set to 512.</p>
<p>Here is a short description of the content of your current directory:</p>
<ul class="simple">
<li><p>The OpenNMT-py repository.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">replicate_vicuna.yaml</span></code> file.</p></li>
<li><p>A subdirectory named “llama” with the llama chekpoints.</p></li>
<li><p>The converted llama7B checkpoint (<code class="docutils literal notranslate"><span class="pre">llama7B-vicuna-onmt</span></code>) and the vocabulary (<code class="docutils literal notranslate"><span class="pre">vocab.txt</span></code>) that will be genenerated with OpenNMT tools.</p></li>
<li><p>A subdirectory named “dataAI” with the datasets for the finetuning.</p></li>
<li><p>A subdirectory named “finetuned_llama7B” that will contain the finetuning samples, tensorboard logs and checkpoints.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">translate_opts.yaml</span></code> file with the translation options for the inference with <code class="docutils literal notranslate"><span class="pre">OpenNMT-py/onmt/bin/translate.py</span></code>.</p></li>
<li><p>A subdirectory named “inputs” containing the <code class="docutils literal notranslate"><span class="pre">input_examples.txt</span></code> file with the input examples for the inference.</p></li>
<li><p>A subdirectory named “outputs” that will contain the inferred outputs of the finetuned model.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">chatbot.py</span></code> script (for the ctranslate2 inference with a gradio application).</p></li>
</ul>
<section id="dependencies">
<h2>Dependencies<a class="headerlink" href="#dependencies" title="Permalink to this heading">¶</a></h2>
<p>Apex is highly recommended to have fast performance.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/NVIDIA/apex
<span class="nb">cd</span><span class="w"> </span>apex
pip3<span class="w"> </span>install<span class="w"> </span>-v<span class="w"> </span>--disable-pip-version-check<span class="w"> </span>--no-cache-dir<span class="w"> </span>--global-option<span class="o">=</span><span class="s2">&quot;--cpp_ext&quot;</span><span class="w"> </span>--global-option<span class="o">=</span><span class="s2">&quot;--cuda_ext&quot;</span><span class="w"> </span>--global-option<span class="o">=</span><span class="s2">&quot;--deprecated_fused_adam&quot;</span><span class="w"> </span>--global-option<span class="o">=</span><span class="s2">&quot;--xentropy&quot;</span><span class="w"> </span>--global-option<span class="o">=</span><span class="s2">&quot;--fast_multihead_attn&quot;</span><span class="w"> </span>./
<span class="nb">cd</span><span class="w"> </span>..
</pre></div>
</div>
<p>You must also have gradio and ctranslate2 installed in your environment:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>gradio
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">ctranslate2</span><span class="o">==</span><span class="m">3</span>.14.0
</pre></div>
</div>
</section>
<section id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this heading">¶</a></h2>
<section id="checkpoints">
<h3>Checkpoints<a class="headerlink" href="#checkpoints" title="Permalink to this heading">¶</a></h3>
<p>The procedure to retrieve the llama checkpoints as well the llama legacy sentencepiece tokenizer is described on the official llama repository:  https://github.com/facebookresearch/llama/</p>
<p>Let us save them in a local folder that we will name “llama”.</p>
<p>We need to convert the llama 7B checkpoint to the <code class="docutils literal notranslate"><span class="pre">onmt</span></code> format, using the <code class="docutils literal notranslate"><span class="pre">convert_llama.py</span></code> tool:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>OpenNMT-py/tools/convert_llama.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_dir<span class="w"> </span>llama/7B/<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tokenizer_model<span class="w"> </span>llama/tokenizer.model<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output<span class="w"> </span>llama7B-vicuna-onmt
</pre></div>
</div>
<p>The converted checkpoint is named <code class="docutils literal notranslate"><span class="pre">llama7B-vicuna-onmt</span></code>.</p>
</section>
<section id="vocabulary">
<h3>Vocabulary<a class="headerlink" href="#vocabulary" title="Permalink to this heading">¶</a></h3>
<p>As the subword model is a sentencepiece model, the vocabulary can be retrieved from the tokenizer. The <code class="docutils literal notranslate"><span class="pre">convert_llama.py</span></code> script saved a copy of the vocabulary with slight modifications but you can also extract the vocabulary from the newly created checkpoint as follow:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>OpenNMT-py/tools/extract_vocabulary.py<span class="w"> </span>-model<span class="w"> </span>llama7B-vicuna-onmt<span class="w"> </span>-out_file<span class="w"> </span>vocab.txt<span class="w"> </span>-side<span class="w"> </span>src
</pre></div>
</div>
</section>
<section id="datasets">
<h3>Datasets<a class="headerlink" href="#datasets" title="Permalink to this heading">¶</a></h3>
<p>The original <a class="reference external" href="https://raw.githubusercontent.com/gururise/AlpacaDataCleaned/main/alpaca_data_cleaned.json"><em>alpaca</em></a> and <em>vicuna</em> datasets are JSON files. This</p>
<p>Here is the first element of the original alpaca_data.json dataset :</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="p">{</span>
<span class="w">        </span><span class="nt">&quot;instruction&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;Give three tips for staying healthy.&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;input&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;&quot;</span><span class="p">,</span>
<span class="w">        </span><span class="nt">&quot;output&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.&quot;</span>
<span class="w">    </span><span class="p">},</span>
</pre></div>
</div>
<p>The <em>vicuna</em> dataset</p>
<p><strong>The datasets that will be used in this tutorial are slightly modified versions of the original datasets.</strong>
They have been flattened into plain text files. Moreover all occurences of the “\n” symbol, which acts as example break in the OpenNMT world, have been replaced with ‘｟newline｠’.</p>
<p>The onmt datasets can be retrieved at the links below:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://opennmt-models.s3.amazonaws.com/llama/alpaca_clean.txt">alpaca</a> (51751 examples)</p></li>
<li><p><a class="reference external" href="https://opennmt-models.s3.amazonaws.com/llama/sharegpt.txt">vicuna</a> (28800 examples)</p></li>
</ul>
<p>Let us save them in a  local folder that we will name <code class="docutils literal notranslate"><span class="pre">dataAI</span></code>.</p>
<p>Each example is a prompt that contains:</p>
<ul class="simple">
<li><p>a short description of the task</p></li>
<li><p>an instrunction following the pattern <code class="docutils literal notranslate"><span class="pre">###</span> <span class="pre">Instruction</span></code></p></li>
<li><p>a proposal of answer following the pattern <code class="docutils literal notranslate"><span class="pre">###</span> <span class="pre">Response</span></code></p></li>
</ul>
<p>Here is the first example in the onmt alpaca dataset:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Below is an instruction that describes a task. Write a response that appropriately completes the request.｟newline｠｟newline｠### Instruction:｟newline｠Give three tips for staying healthy.｟newline｠｟newline｠### Response:｟newline｠1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.｟newline｠｟newline｠2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.｟newline｠｟newline｠3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.
</pre></div>
</div>
</section>
</section>
<section id="finetuning">
<h2>Finetuning<a class="headerlink" href="#finetuning" title="Permalink to this heading">¶</a></h2>
<p>We provide an example of a finetuning configuration (<code class="docutils literal notranslate"><span class="pre">replicate_alpaca.yaml</span></code>). To enable the application of the LoRa method to the attention layers, the options of the checkpoint need to be overriden.</p>
<p>The finetuning can be launched with this command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>nohup<span class="w"> </span>python3<span class="w"> </span>OpenNMT-py/onmt/bin/train.py<span class="w"> </span>-config<span class="w"> </span>replicate_alpaca-vicuna.yaml<span class="w"> </span>&gt;<span class="w"> </span>finetenune-llama7B-vicuna-onmt.log<span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
<p>We can start by generating some samples (by turning <code class="docutils literal notranslate"><span class="pre">dump_samples</span></code> to True and <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> to a strictly positive value).</p>
<p>It is worth noting that the he sentencepiece vocabulary does not map the custom substring ｟newline｠with a specific token. However it maps the new line symbol ‘\n’ with the token ‘&lt;0x0A&gt;’. To handle properly our datasets without changing the vocabulary and training new embddings from scratch, the Tokenize transform replaces on-the-fly the token ‘｟newline｠’ token with  ‘&lt;0x0A&gt;’.</p>
<p>For instance the first training example is transformed in:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>▁Below ▁is ▁an ▁instruction ▁that ▁describes ▁a ▁task . ▁Write ▁a ▁response ▁that ▁appropri ately ▁comple tes ▁the ▁request . &lt;0x0A&gt; &lt;0x0A&gt; ## # ▁Inst ruction : &lt;0x0A&gt; G ive ▁three ▁tips ▁for ▁stay ing ▁health y . &lt;0x0A&gt; &lt;0x0A&gt; ## # ▁Response : &lt;0x0A&gt; 1 . ▁E at ▁a ▁bal anced ▁and ▁nut rit ious ▁di et : ▁Make ▁sure ▁your ▁me als ▁are ▁inclus ive ▁of ▁a ▁variety ▁of ▁f ruits ▁and ▁veget ables , ▁lean ▁protein , ▁whole ▁gra ins , ▁and ▁health y ▁f ats . ▁This ▁helps ▁to ▁provide ▁your ▁body ▁with ▁the ▁essential ▁nut ri ents ▁to ▁function ▁at ▁its ▁best ▁and ▁can ▁help ▁prevent ▁chron ic ▁dise ases . &lt;0x0A&gt; &lt;0x0A&gt; 2 . ▁Eng age ▁in ▁regular ▁physical ▁activity : ▁Ex erc ise ▁is ▁cru cial ▁for ▁maintain ing ▁strong ▁b ones , ▁mus cles , ▁and ▁card i ov asc ular ▁health . ▁A im ▁for ▁at ▁least ▁ 1 5 0 ▁minutes ▁of ▁moder ate ▁aer ob ic ▁exercise ▁or ▁ 7 5 ▁minutes ▁of ▁vig orous ▁exercise ▁each ▁week . &lt;0x0A&gt; &lt;0x0A&gt; 3 . ▁Get ▁enough ▁sleep : ▁Getting ▁enough ▁quality ▁sleep ▁is ▁cru cial ▁for ▁physical ▁and ▁mental ▁well - be ing . ▁It ▁helps ▁to ▁reg ulate ▁m ood , ▁improve ▁cogn itive ▁function , ▁and ▁supports ▁health y ▁growth ▁and ▁imm une ▁function . ▁A im ▁for ▁ 7 - 9 ▁hours ▁of ▁sleep ▁each ▁night .
</pre></div>
</div>
</section>
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this heading">¶</a></h2>
<section id="concatenation-of-the-checkpoints">
<h3>Concatenation of the checkpoints<a class="headerlink" href="#concatenation-of-the-checkpoints" title="Permalink to this heading">¶</a></h3>
<p>As we applied the LoRa method, we first need to merge the finetuned <code class="docutils literal notranslate"><span class="pre">llama7B-vicuna-onmt.pt</span></code> checkpoint in the original <code class="docutils literal notranslate"><span class="pre">llama7B-onmt.pt</span></code> model, using the <code class="docutils literal notranslate"><span class="pre">lora_weights.py</span> <span class="pre">tool</span></code>. :</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python3<span class="w"> </span>OpenNMT-py/tools/lora_weights.py<span class="se">\</span>
<span class="w">    </span>--action<span class="w"> </span>merge<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--base_model<span class="w"> </span>llama7B-vicuna-onmt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--lora_weights<span class="w"> </span>finetuned_llama7B/llama7B-vicuna-onmt_step_4000.pt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output<span class="w"> </span>finetuned_llama7B/llama7B-vicuna-onmt_step_4000.concat.pt
</pre></div>
</div>
</section>
<section id="input-examples">
<h3>Input examples<a class="headerlink" href="#input-examples" title="Permalink to this heading">¶</a></h3>
<p>The inputs need to follow the same pattern used in the finetuning examples.</p>
<p>Let us create an “inputs” folder and save inside it the file named <code class="docutils literal notranslate"><span class="pre">input_examples.txt</span></code>.</p>
</section>
<section id="inference-with-translate-py">
<h3>Inference with <code class="docutils literal notranslate"><span class="pre">translate.py</span></code><a class="headerlink" href="#inference-with-translate-py" title="Permalink to this heading">¶</a></h3>
<p>Let us create an “outputs” folder.</p>
<p>To obtain the model’s inference you can run this command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>nohup<span class="w"> </span>python3<span class="w"> </span>OpenNMT-py/onmt/bin/translate.py<span class="se">\</span>
<span class="w">    </span>-model<span class="w"> </span>finetuned_llama7B/llama7B-vicuna-onmt_step_4000.concat.pt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-src<span class="w"> </span>inputs/input_examples.txt<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-output<span class="w"> </span>outputs/examples_llama7B-vicuna-onmt_step_4000.concat.txt<span class="w">  </span><span class="se">\</span>
<span class="w">    </span>-config<span class="w"> </span>translate_opts.yaml<span class="w"> </span>&gt;<span class="w"> </span>infer.log<span class="w"> </span><span class="p">&amp;</span><span class="w"> </span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">translate_opts.yaml</span></code> is the provided config with the translation options.
You can test other decoding methods and paramaters.</p>
<p>We have also provided a gradio application to chat with the model.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>gradio<span class="w"> </span>chatbot.py
</pre></div>
</div>
<p>You must use <code class="docutils literal notranslate"><span class="pre">inf_type</span> <span class="pre">=</span> <span class="pre">&quot;-py&quot;</span></code> at the beginning of the <code class="docutils literal notranslate"><span class="pre">chatbot.py</span></code> script.</p>
</section>
<section id="inference-with-ctranslate">
<h3>Inference with <code class="docutils literal notranslate"><span class="pre">CTranslate</span></code><a class="headerlink" href="#inference-with-ctranslate" title="Permalink to this heading">¶</a></h3>
<p>First we need to do the conversion to the ctranslate2 format.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shell</span>
<span class="n">python3</span> <span class="n">OpenNMT</span><span class="o">-</span><span class="n">py</span><span class="o">/</span><span class="n">onmt</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">release_model</span><span class="o">.</span><span class="n">py</span> \
    <span class="o">--</span><span class="n">model</span> <span class="n">finetuned_llama7B</span><span class="o">/</span><span class="n">llama7B</span><span class="o">-</span><span class="n">vicuna</span><span class="o">-</span><span class="n">onmt_step_4000</span><span class="o">.</span><span class="n">concat</span><span class="o">.</span><span class="n">pt</span> \
    <span class="o">--</span><span class="n">output</span> <span class="n">finetuned_llama7B</span><span class="o">/</span><span class="n">llama7B</span><span class="o">-</span><span class="n">vicuna</span><span class="o">-</span><span class="n">onmt_step_4000</span><span class="o">.</span><span class="n">concat_CT2</span> \
    <span class="o">--</span><span class="nb">format</span> <span class="n">ctranslate2</span> \
    <span class="o">--</span><span class="n">quantization</span> <span class="n">int8_float16</span>
</pre></div>
</div>
<p>You can chat with the model using <code class="docutils literal notranslate"><span class="pre">inf_type</span> <span class="pre">=</span> <span class="pre">&quot;ct2&quot;</span></code> at the beginning of the <code class="docutils literal notranslate"><span class="pre">chatbot.py</span></code> script.</p>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../options/build_vocab.html" class="btn btn-neutral float-right" title="Build Vocab" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../ggnn/GGNN.html" class="btn btn-neutral float-left" title="Gated Graph Neural Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2023, OpenNMT

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>