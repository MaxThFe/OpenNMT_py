

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Framework &mdash; OpenNMT-py  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Modules" href="onmt.modules.html" />
    <link rel="prev" title="Server" href="options/server.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> OpenNMT-py
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="ref.html">References</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/Translation.html">Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/Summarization.html">Summarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/LanguageModelGeneration.html">Language Model Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/GGNN.html">Gated Graph Sequence Neural Networks</a></li>
</ul>
<p class="caption"><span class="caption-text">Scripts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="options/build_vocab.html">Build Vocab</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/translate.html">Translate</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/server.html">Server</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Framework</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#trainer">Trainer</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loss">Loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizer">Optimizer</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="onmt.modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.translation.html">Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.translate.translation_server.html">Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.inputters.html">Data Loaders</a></li>
</ul>
<p class="caption"><span class="caption-text">Legacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="legacy/FAQ.html">FAQ (Legacy version)</a></li>
<li class="toctree-l1"><a class="reference internal" href="legacy/im2text.html">Image to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="legacy/speech2text.html">Speech to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="legacy/vid2text.html">Video to Text</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenNMT-py</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Framework</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/onmt.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="framework">
<h1>Framework<a class="headerlink" href="#framework" title="Permalink to this headline">¶</a></h1>
<div class="section" id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.models.BaseModel">
<em class="property">class </em><code class="sig-prename descclassname">onmt.models.</code><code class="sig-name descname">BaseModel</code><span class="sig-paren">(</span><em class="sig-param">encoder</em>, <em class="sig-param">decoder</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/models/model.html#BaseModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.models.BaseModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Core trainable object in OpenNMT. Implements a trainable interface
for a simple, generic encoder / decoder or decoder only model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>encoder</strong> (<a class="reference internal" href="onmt.modules.html#onmt.encoders.EncoderBase" title="onmt.encoders.EncoderBase"><em>onmt.encoders.EncoderBase</em></a>) – an encoder object</p></li>
<li><p><strong>decoder</strong> (<a class="reference internal" href="onmt.modules.html#onmt.decoders.DecoderBase" title="onmt.decoders.DecoderBase"><em>onmt.decoders.DecoderBase</em></a>) – a decoder object</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="onmt.models.BaseModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">tgt</em>, <em class="sig-param">src_len</em>, <em class="sig-param">bptt=False</em>, <em class="sig-param">with_align=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/models/model.html#BaseModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.models.BaseModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Forward propagate a <cite>src</cite> and <cite>tgt</cite> pair for training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>src</strong> (<em>Tensor</em>) – A source sequence passed to encoder.
Typically for input this will be a padded <cite>LongTensor</cite>
of size <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">len,</span> <span class="pre">features)</span></code>. However, may be an
image or other generic input depending on encoder.</p></li>
<li><p><strong>tgt</strong> (<em>LongTensor</em>) – A target sequence passed to decoder.
Size <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">tgt_len,</span> <span class="pre">features)</span></code>.</p></li>
<li><p><strong>src_len</strong> (<em>LongTensor</em>) – The src lengths, pre-padding <code class="docutils literal notranslate"><span class="pre">(batch,)</span></code>.</p></li>
<li><p><strong>bptt</strong> (<em>Boolean</em>) – A flag indicating if truncated bptt is set.
If bptt is false then init decoder state.</p></li>
<li><p><strong>with_align</strong> (<em>Boolean</em>) – A flag indicating whether output alignment,
Only valid for transformer decoder.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p>decoder output <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">tgt_len,</span> <span class="pre">hidden)</span></code></p></li>
<li><p>dictionary of attention weights <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">tgt_len,</span> <span class="pre">src_len)</span></code></p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(FloatTensor, dict[str, FloatTensor])</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.models.NMTModel">
<em class="property">class </em><code class="sig-prename descclassname">onmt.models.</code><code class="sig-name descname">NMTModel</code><span class="sig-paren">(</span><em class="sig-param">encoder</em>, <em class="sig-param">decoder</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/models/model.html#NMTModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.models.NMTModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">onmt.models.model.BaseModel</span></code></p>
<p>NMTModel Class
See <a class="reference internal" href="#onmt.models.BaseModel" title="onmt.models.BaseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseModel</span></code></a> for options.</p>
<dl class="method">
<dt id="onmt.models.NMTModel.count_parameters">
<code class="sig-name descname">count_parameters</code><span class="sig-paren">(</span><em class="sig-param">log=&lt;built-in function print&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/models/model.html#NMTModel.count_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.models.NMTModel.count_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Count number of parameters in model (&amp; print with <cite>log</cite> callback).</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p>encoder side parameter count</p></li>
<li><p>decoder side parameter count</p></li>
</ul>
</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>(int, int)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.models.NMTModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">tgt</em>, <em class="sig-param">src_len</em>, <em class="sig-param">bptt=False</em>, <em class="sig-param">with_align=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/models/model.html#NMTModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.models.NMTModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>An NMTModel forward the src side to the encoder.
Then the output of encoder <code class="docutils literal notranslate"><span class="pre">enc_out</span></code> is forwarded to the
decoder along with the target excluding the last token.
The decoder state is initiliazed with:</p>
<blockquote>
<div><ul class="simple">
<li><p>enc_final_hs in the case of RNNs</p></li>
<li><p>enc_out + enc_final_hs in the case of CNNs</p></li>
<li><p>src in the case of Transformer</p></li>
</ul>
</div></blockquote>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.models.LanguageModel">
<em class="property">class </em><code class="sig-prename descclassname">onmt.models.</code><code class="sig-name descname">LanguageModel</code><span class="sig-paren">(</span><em class="sig-param">encoder=None</em>, <em class="sig-param">decoder=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/models/model.html#LanguageModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.models.LanguageModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">onmt.models.model.BaseModel</span></code></p>
<p>NMTModel Class
Currently TransformerLMDecoder is the only LM decoder implemented
:param decoder: a transformer decoder
:type decoder: onmt.decoders.TransformerLMDecoder</p>
<dl class="method">
<dt id="onmt.models.LanguageModel.count_parameters">
<code class="sig-name descname">count_parameters</code><span class="sig-paren">(</span><em class="sig-param">log=&lt;built-in function print&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/models/model.html#LanguageModel.count_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.models.LanguageModel.count_parameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Count number of parameters in model (&amp; print with <cite>log</cite> callback).
:returns:</p>
<blockquote>
<div><ul class="simple">
<li><p>encoder side parameter count</p></li>
<li><p>decoder side parameter count</p></li>
</ul>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(int, int)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.models.LanguageModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">src</em>, <em class="sig-param">tgt</em>, <em class="sig-param">src_len</em>, <em class="sig-param">bptt=False</em>, <em class="sig-param">with_align=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/models/model.html#LanguageModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.models.LanguageModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>A LanguageModel forward the src side to the decoder along
with the source lengths vector. It is a decoder only LM (cf GPT-2)</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="trainer">
<h2>Trainer<a class="headerlink" href="#trainer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.Trainer">
<em class="property">class </em><code class="sig-prename descclassname">onmt.</code><code class="sig-name descname">Trainer</code><span class="sig-paren">(</span><em class="sig-param">model, train_loss, valid_loss, scoring_preparator, train_scorers, valid_scorers, optim, trunc_size=0, norm_method='sents', accum_count=[1], accum_steps=[0], n_gpu=1, gpu_rank=1, train_eval_steps=200, report_manager=None, with_align=False, model_saver=None, average_decay=0, average_every=1, model_dtype='fp32', earlystopper=None, dropout=[0.3], attention_dropout=[0.1], dropout_steps=[0]</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/trainer.html#Trainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.Trainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Class that controls the training process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">onmt.models.model.NMTModel</span></code>) – translation model
to train</p></li>
<li><p><strong>train_loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">onmt.utils.loss.LossComputeBase</span></code>) – training loss computation</p></li>
<li><p><strong>valid_loss</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">onmt.utils.loss.LossComputeBase</span></code>) – training loss computation</p></li>
<li><p><strong>scoring_preparator</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">onmt.translate.utils.ScoringPreparator</span></code>) – preparator for the calculation of metrics via the
training_eval_handler method</p></li>
<li><p><strong>train_scorers</strong> (<em>dict</em>) – keeps in memory the current values
of the training metrics</p></li>
<li><p><strong>valid_scorers</strong> (<em>dict</em>) – keeps in memory the current values
of the validation metrics</p></li>
<li><p><strong>optim</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">onmt.utils.optimizers.Optimizer</span></code>) – the optimizer responsible for update</p></li>
<li><p><strong>trunc_size</strong> (<em>int</em>) – length of truncated back propagation through time</p></li>
<li><p><strong>accum_count</strong> (<em>list</em>) – accumulate gradients this many times.</p></li>
<li><p><strong>accum_steps</strong> (<em>list</em>) – steps for accum gradients changes.</p></li>
<li><p><strong>n_gpu</strong> (<em>int</em>) – number of gpu.</p></li>
<li><p><strong>gpu_rank</strong> (<em>int</em>) – ordinal rank of the gpu in the list.</p></li>
<li><p><strong>train_eval_steps</strong> (<em>int</em>) – process a validation every x steps.</p></li>
<li><p><strong>report_manager</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">onmt.utils.ReportMgrBase</span></code>) – the object that creates reports, or None</p></li>
<li><p><strong>with_align</strong> (<em>bool</em>) – whether to jointly lear alignment (Transformer)</p></li>
<li><p><strong>model_saver</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">onmt.models.ModelSaverBase</span></code>) – the saver is
used to save a checkpoint.
Thus nothing will be saved if this parameter is None.</p></li>
<li><p><strong>average_decay</strong> (<em>float</em>) – cf opt.average_decay</p></li>
<li><p><strong>average_every</strong> (<em>int</em>) – average model every x steps.</p></li>
<li><p><strong>model_dtype</strong> (<em>str</em>) – fp32 or fp16.</p></li>
<li><p><strong>earlystopper</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">onmt.utils.EarlyStopping</span></code>) – add early
stopping mecanism</p></li>
<li><p><strong>dropout</strong> (<em>float</em>) – dropout value in RNN or FF layers.</p></li>
<li><p><strong>attention_dropout</strong> (<em>float</em>) – dropaout in attention layers.</p></li>
<li><p><strong>dropout_steps</strong> (<em>list</em>) – dropout values scheduling in steps.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="onmt.Trainer.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">train_iter</em>, <em class="sig-param">train_steps</em>, <em class="sig-param">save_checkpoint_steps=5000</em>, <em class="sig-param">valid_iter=None</em>, <em class="sig-param">valid_steps=10000</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/trainer.html#Trainer.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.Trainer.train" title="Permalink to this definition">¶</a></dt>
<dd><p>The main training loop by iterating over <cite>train_iter</cite> and possibly
running validation on <cite>valid_iter</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>train_iter</strong> – An iterator that returns the next training batch.</p></li>
<li><p><strong>train_steps</strong> – Run training for this many iterations.</p></li>
<li><p><strong>save_checkpoint_steps</strong> – Save a checkpoint every this many
iterations.</p></li>
<li><p><strong>valid_iter</strong> – A generator that returns the next validation batch.</p></li>
<li><p><strong>valid_steps</strong> – Run evaluation every this many iterations.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>training loss statistics</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nmt.Statistics</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.Trainer.validate">
<code class="sig-name descname">validate</code><span class="sig-paren">(</span><em class="sig-param">valid_iter</em>, <em class="sig-param">moving_average=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/trainer.html#Trainer.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.Trainer.validate" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Validate model.</dt><dd><p>valid_iter: validate data iterator</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>validation loss statistics</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">nmt.Statistics</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.utils.Statistics">
<em class="property">class </em><code class="sig-prename descclassname">onmt.utils.</code><code class="sig-name descname">Statistics</code><span class="sig-paren">(</span><em class="sig-param">loss=0</em>, <em class="sig-param">n_batchs=0</em>, <em class="sig-param">n_sents=0</em>, <em class="sig-param">n_words=0</em>, <em class="sig-param">n_correct=0</em>, <em class="sig-param">computed_metrics={}</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Accumulator for loss statistics.
Currently calculates:</p>
<ul class="simple">
<li><p>accuracy</p></li>
<li><p>perplexity</p></li>
<li><p>elapsed time</p></li>
</ul>
<dl class="method">
<dt id="onmt.utils.Statistics.accuracy">
<code class="sig-name descname">accuracy</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>compute accuracy</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Statistics.all_gather_stats">
<em class="property">static </em><code class="sig-name descname">all_gather_stats</code><span class="sig-paren">(</span><em class="sig-param">stat</em>, <em class="sig-param">max_size=4096</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.all_gather_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.all_gather_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Gather a <cite>Statistics</cite> object accross multiple process/nodes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stat</strong><strong>(</strong> – obj:Statistics): the statistics object to gather
accross all processes/nodes</p></li>
<li><p><strong>max_size</strong> (<em>int</em>) – max buffer size to use</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><cite>Statistics</cite>, the update stats object</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Statistics.all_gather_stats_list">
<em class="property">static </em><code class="sig-name descname">all_gather_stats_list</code><span class="sig-paren">(</span><em class="sig-param">stat_list</em>, <em class="sig-param">max_size=4096</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.all_gather_stats_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.all_gather_stats_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Gather a <cite>Statistics</cite> list accross all processes/nodes</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stat_list</strong> (list([<cite>Statistics</cite>])) – list of statistics objects to
gather accross all processes/nodes</p></li>
<li><p><strong>max_size</strong> (<em>int</em>) – max buffer size to use</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>list of updated stats</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>our_stats(list([<cite>Statistics</cite>]))</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Statistics.elapsed_time">
<code class="sig-name descname">elapsed_time</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.elapsed_time"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.elapsed_time" title="Permalink to this definition">¶</a></dt>
<dd><p>compute elapsed time</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Statistics.log_tensorboard">
<code class="sig-name descname">log_tensorboard</code><span class="sig-paren">(</span><em class="sig-param">prefix</em>, <em class="sig-param">writer</em>, <em class="sig-param">learning_rate</em>, <em class="sig-param">patience</em>, <em class="sig-param">step</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.log_tensorboard"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.log_tensorboard" title="Permalink to this definition">¶</a></dt>
<dd><p>display statistics to tensorboard</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Statistics.output">
<code class="sig-name descname">output</code><span class="sig-paren">(</span><em class="sig-param">step</em>, <em class="sig-param">num_steps</em>, <em class="sig-param">learning_rate</em>, <em class="sig-param">start</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.output"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.output" title="Permalink to this definition">¶</a></dt>
<dd><p>Write out statistics to stdout.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>step</strong> (<em>int</em>) – current step</p></li>
<li><p><strong>n_batch</strong> (<em>int</em>) – total batches</p></li>
<li><p><strong>start</strong> (<em>int</em>) – start time of step.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Statistics.ppl">
<code class="sig-name descname">ppl</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.ppl"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.ppl" title="Permalink to this definition">¶</a></dt>
<dd><p>compute perplexity</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Statistics.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param">stat</em>, <em class="sig-param">update_n_src_words=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Update statistics by suming values with another <cite>Statistics</cite> object</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stat</strong> – another statistic object</p></li>
<li><p><strong>update_n_src_words</strong> (<em>bool</em>) – whether to update (sum) <cite>n_src_words</cite>
or not</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Statistics.xent">
<code class="sig-name descname">xent</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/statistics.html#Statistics.xent"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Statistics.xent" title="Permalink to this definition">¶</a></dt>
<dd><p>compute cross entropy</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="loss">
<h2>Loss<a class="headerlink" href="#loss" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.utils.loss.LossCompute">
<em class="property">class </em><code class="sig-prename descclassname">onmt.utils.loss.</code><code class="sig-name descname">LossCompute</code><span class="sig-paren">(</span><em class="sig-param">criterion</em>, <em class="sig-param">generator</em>, <em class="sig-param">copy_attn=False</em>, <em class="sig-param">lambda_coverage=0.0</em>, <em class="sig-param">lambda_align=0.0</em>, <em class="sig-param">tgt_shift_index=1</em>, <em class="sig-param">vocab=None</em>, <em class="sig-param">lm_generator=None</em>, <em class="sig-param">lm_prior_lambda=None</em>, <em class="sig-param">lm_prior_tau=None</em>, <em class="sig-param">lm_prior_model=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/loss.html#LossCompute"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.loss.LossCompute" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Class for managing efficient loss computation. Handles
accumulating multiple loss computations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.</span> <span class="pre">loss</span> <span class="pre">function</span></code>) – NLLoss or customed loss</p></li>
<li><p><strong>generator</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">nn.Module</span></code>) – </p></li>
<li><p><strong>copy_attn</strong> (<em>bool</em>) – whether copy attention mechanism is on/off</p></li>
<li><p><strong>lambda_coverage</strong> – Hyper-param to apply coverage attention if any</p></li>
<li><p><strong>lambda_align</strong> – Hyper-param for alignment loss</p></li>
<li><p><strong>tgt_shift_index</strong> (<em>int</em>) – 1 for NMT, 0 for LM</p></li>
<li><p><strong>vocab</strong> – target vocab (for copy attention score calculation)
module that maps the output of the decoder to a
distribution over the target vocabulary.</p></li>
<li><p><strong>lm_generator</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">ctranslate2.Generator</span></code>) – LM Generator</p></li>
<li><p><strong>lm_prior_lambda</strong> (<em>float</em>) – weight of LM model in loss</p></li>
<li><p><strong>lm_prior_tau</strong> (<em>float</em>) – scaler for LM loss</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="onmt.utils.loss.LossCompute.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">batch</em>, <em class="sig-param">output</em>, <em class="sig-param">attns</em>, <em class="sig-param">trunc_start=0</em>, <em class="sig-param">trunc_size=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/loss.html#LossCompute.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.loss.LossCompute.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the forward loss, supports truncated BPTT for long
sequences by taking a range in the decoder output sequence to
back propagate in.
Range is from <cite>(trunc_start, trunc_start + trunc_size)</cite>.
Truncation is an approximate efficiency trick to relieve the
memory required in the RNN buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong> (<em>batch</em>) – batch of labeled examples</p></li>
<li><p><strong>output</strong> (<code class="xref py py-obj docutils literal notranslate"><span class="pre">FloatTensor</span></code>) – output of decoder model <code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">tgt_len,</span> <span class="pre">hidden)</span></code></p></li>
<li><p><strong>attns</strong> (<em>dict</em>) – dictionary of attention weights
<code class="docutils literal notranslate"><span class="pre">(batch,</span> <span class="pre">tgt_len,</span> <span class="pre">src_len)</span></code></p></li>
<li><p><strong>trunc_start</strong> (<em>int</em>) – starting position of truncation window</p></li>
<li><p><strong>trunc_size</strong> (<em>int</em>) – length of truncation window</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple with the loss and a <a class="reference internal" href="#onmt.utils.Statistics" title="onmt.utils.Statistics"><code class="xref py py-obj docutils literal notranslate"><span class="pre">onmt.utils.Statistics</span></code></a> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.loss.LossCompute.from_opts">
<em class="property">classmethod </em><code class="sig-name descname">from_opts</code><span class="sig-paren">(</span><em class="sig-param">opt</em>, <em class="sig-param">model</em>, <em class="sig-param">vocab</em>, <em class="sig-param">train=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/loss.html#LossCompute.from_opts"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.loss.LossCompute.from_opts" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a subclass which wraps around an nn.Module subclass
(such as nn.NLLLoss) which defines the loss criterion. The LossCompute
object passes relevant data to a Statistics object which handles
training/validation logging.
The Criterion and LossCompute options are triggered by opt settings.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="optimizer">
<h2>Optimizer<a class="headerlink" href="#optimizer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="onmt.utils.Optimizer">
<em class="property">class </em><code class="sig-prename descclassname">onmt.utils.</code><code class="sig-name descname">Optimizer</code><span class="sig-paren">(</span><em class="sig-param">optimizer</em>, <em class="sig-param">learning_rate</em>, <em class="sig-param">learning_rate_decay_fn=None</em>, <em class="sig-param">max_grad_norm=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#Optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Controller class for optimization. Mostly a thin
wrapper for <cite>optim</cite>, but also useful for implementing
rate scheduling beyond what is currently available.
Also implements necessary methods for training RNNs such
as grad manipulations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong> – A <code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> instance.</p></li>
<li><p><strong>learning_rate</strong> – The initial learning rate.</p></li>
<li><p><strong>learning_rate_decay_fn</strong> – An optional callable taking the current step
as argument and return a learning rate scaling factor.</p></li>
<li><p><strong>max_grad_norm</strong> – Clip gradients to this global norm.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="onmt.utils.Optimizer.amp">
<em class="property">property </em><code class="sig-name descname">amp</code><a class="headerlink" href="#onmt.utils.Optimizer.amp" title="Permalink to this definition">¶</a></dt>
<dd><p>True if use torch amp mix precision training.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Optimizer.backward">
<code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param">loss</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#Optimizer.backward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Optimizer.backward" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper for backward pass. Some optimizer requires ownership of the
backward pass.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Optimizer.from_opt">
<em class="property">classmethod </em><code class="sig-name descname">from_opt</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">opt</em>, <em class="sig-param">checkpoint=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#Optimizer.from_opt"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Optimizer.from_opt" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds the optimizer from options.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cls</strong> – The <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code> class to instantiate.</p></li>
<li><p><strong>model</strong> – The model to optimize.</p></li>
<li><p><strong>opt</strong> – The dict of user options.</p></li>
<li><p><strong>checkpoint</strong> – An optional checkpoint to load states from.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An <code class="docutils literal notranslate"><span class="pre">Optimizer</span></code> instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Optimizer.learning_rate">
<code class="sig-name descname">learning_rate</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#Optimizer.learning_rate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Optimizer.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current learning rate.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Optimizer.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#Optimizer.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Optimizer.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the model parameters based on current gradients.</p>
<p>Optionally, will employ gradient modification or update learning
rate.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Optimizer.training_step">
<em class="property">property </em><code class="sig-name descname">training_step</code><a class="headerlink" href="#onmt.utils.Optimizer.training_step" title="Permalink to this definition">¶</a></dt>
<dd><p>The current training step.</p>
</dd></dl>

<dl class="method">
<dt id="onmt.utils.Optimizer.zero_grad">
<code class="sig-name descname">zero_grad</code><span class="sig-paren">(</span><em class="sig-param">set_to_none=True</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#Optimizer.zero_grad"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.Optimizer.zero_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Zero the gradients of optimized parameters.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.utils.AdaFactor">
<em class="property">class </em><code class="sig-prename descclassname">onmt.utils.</code><code class="sig-name descname">AdaFactor</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=None</em>, <em class="sig-param">beta1=0.9</em>, <em class="sig-param">beta2=0.999</em>, <em class="sig-param">eps1=1e-30</em>, <em class="sig-param">eps2=0.001</em>, <em class="sig-param">cliping_threshold=1</em>, <em class="sig-param">non_constant_decay=True</em>, <em class="sig-param">enable_factorization=True</em>, <em class="sig-param">ams_grad=True</em>, <em class="sig-param">weight_decay=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#AdaFactor"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.AdaFactor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<dl class="method">
<dt id="onmt.utils.AdaFactor.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">closure=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#AdaFactor.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.AdaFactor.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step (parameter update).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>closure</strong> (<em>Callable</em>) – A closure that reevaluates the model and
returns the loss. Optional for most optimizers.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unless otherwise specified, this function should not modify the
<code class="docutils literal notranslate"><span class="pre">.grad</span></code> field of the parameters.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="onmt.utils.FusedAdam">
<em class="property">class </em><code class="sig-prename descclassname">onmt.utils.</code><code class="sig-name descname">FusedAdam</code><span class="sig-paren">(</span><em class="sig-param">params</em>, <em class="sig-param">lr=0.001</em>, <em class="sig-param">bias_correction=True</em>, <em class="sig-param">betas=(0.9</em>, <em class="sig-param">0.999)</em>, <em class="sig-param">eps=1e-08</em>, <em class="sig-param">eps_inside_sqrt=False</em>, <em class="sig-param">weight_decay=0.0</em>, <em class="sig-param">max_grad_norm=0.0</em>, <em class="sig-param">amsgrad=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#FusedAdam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.FusedAdam" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.optimizer.Optimizer</span></code></p>
<dl class="simple">
<dt>Implements Adam algorithm. Currently GPU-only.</dt><dd><p>Requires Apex to be installed via
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">setup.py</span> <span class="pre">install</span> <span class="pre">--cuda_ext</span> <span class="pre">--cpp_ext</span></code>.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>params</strong> (<em>iterable</em>) – iterable of parameters to optimize or dicts defining
parameter groups.</p></li>
<li><p><strong>lr</strong> (<em>float</em><em>, </em><em>optional</em>) – learning rate. (default: 1e-3)</p></li>
<li><p><strong>betas</strong> (<em>Tuple</em><em>[</em><em>float</em><em>, </em><em>float</em><em>]</em><em>, </em><em>optional</em>) – coefficients used for computing
running averages of gradient and its square.
(default: (0.9, 0.999))</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>optional</em>) – term added to the denominator to improve
numerical stability. (default: 1e-8)</p></li>
<li><p><strong>weight_decay</strong> (<em>float</em><em>, </em><em>optional</em>) – weight decay (L2 penalty) (default: 0)</p></li>
<li><p><strong>amsgrad</strong> (<em>boolean</em><em>, </em><em>optional</em>) – whether to use the AMSGrad variant of this
algorithm from the paper <a href="#id1"><span class="problematic" id="id2">`On the Convergence of Adam and Beyond`_</span></a>
(default: False) NOT SUPPORTED in FusedAdam!</p></li>
<li><p><strong>eps_inside_sqrt</strong> (<em>boolean</em><em>, </em><em>optional</em>) – in the ‘update parameters’ step,
adds eps to the bias-corrected second moment estimate before
evaluating square root instead of adding it to the square root of
second moment estimate as in the original paper. (default: False)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="onmt.utils.FusedAdam.step">
<code class="sig-name descname">step</code><span class="sig-paren">(</span><em class="sig-param">closure=None</em>, <em class="sig-param">grads=None</em>, <em class="sig-param">output_params=None</em>, <em class="sig-param">scale=1.0</em>, <em class="sig-param">grad_norms=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/onmt/utils/optimizers.html#FusedAdam.step"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#onmt.utils.FusedAdam.step" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs a single optimization step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>closure</strong> (<em>callable</em><em>, </em><em>optional</em>) – A closure that reevaluates the model
and returns the loss.</p></li>
<li><p><strong>grads</strong> (<em>list of tensors</em><em>, </em><em>optional</em>) – weight gradient to use for the
optimizer update. If gradients have type torch.half, parameters
are expected to be in type torch.float. (default: None)</p></li>
<li><p><strong>params</strong> (<em>output</em>) – A reduced precision copy
of the updated weights written out in addition to the regular
updated weights. Have to be of same type as gradients.
(default: None)</p></li>
<li><p><strong>scale</strong> (<em>float</em><em>, </em><em>optional</em>) – factor to divide gradient tensor values
by before applying to weights. (default: 1)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="onmt.modules.html" class="btn btn-neutral float-right" title="Modules" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="options/server.html" class="btn btn-neutral float-left" title="Server" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, OpenNMT

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>